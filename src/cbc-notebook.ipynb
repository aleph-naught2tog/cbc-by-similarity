{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4328064",
   "metadata": {},
   "source": [
    "# Clustering of CBC Data in Madison, WI (1907 - 2024)\n",
    "\n",
    "## Intro\n",
    "\n",
    "The goal here is to see which birds in the CBC have similar patterns. We'll do\n",
    "this by using KMeans clustering over the timeseries datasets. The CBC data is a human-compiled aggregation of observations taken during the CBC every year.\n",
    "\n",
    "We have two facets we can compare: total bird count vs number by party hours. (The latter is an attempt at normalization).\n",
    "\n",
    "The input file is a CSV made using transform code taken from Jer Thorpe from raw CBC data files that contains only the timeseries data and excludes the other facets. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faf06256",
   "metadata": {},
   "source": [
    "## Quick process overview\n",
    "\n",
    "The extremely simplified process is this:\n",
    "\n",
    "1. have some data\n",
    "2. cluster it\n",
    "3. see which data corresponds to which cluster\n",
    "\n",
    "If we spell it out more:\n",
    "\n",
    "1. Have a set of timeseries data: your X is time, your Y is whatever. In our case, our X is the years from 1907 to 2024, and our Y represents how many birds counted (whether by absolute count or party hour)\n",
    "2. Split that timeseries out into a series of dataframes; each one has an X of the exact same year set, and each dataframe corresponds to a *specific bird's data*.\n",
    "   1. Ensure that each dataframe has the exact same set of years for the X's. (or more explicitly, the same LENGTH, but in this case what is more meaningful is the same set of years, which in turn guarantees the same dimensionality for the range of X). In our case, one dataset was missing a year, so we just added it manually.\n",
    "   2. Ensure you're handling any nullish values somehow, otherwise you'll end up with `nan` and all the issues that happen when that propagates. In our case, we coerce to -1.\n",
    "3. Given the range of numbers you're considering for the number of clusters for the KMeans, calculate the inertia(\"the spread/variation of data points around the mean\") for each possible cluster size.\n",
    "4. Pick cluster count at the \"elbow\" out of your inertia data. (the cluster count after which more clusters don't really do much, ie diminishing returns)\n",
    "5. Calculate the labels (clusters) for the all the timeseries data. When then indexed against our list of birds, this tells us which bird data is in what cluster."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3036e20f",
   "metadata": {},
   "source": [
    "## The guts of it\n",
    "\n",
    "### Transforming the data\n",
    "\n",
    "The first thing we need to do is pull in our json, and transform it to dataframes.\n",
    "\n",
    "From this, we get two things: the set of all dataframes by bird, as well as the list of bird names. It's important to note that these correspond by index: that is, `bird_names[0]` is the name of the bird that represents the data at `all_bird_series[0]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c8c6f8f8",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'main'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mmain\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m get_filename\n\u001b[32m      2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtransform_helpers\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m json_to_dataframes\n\u001b[32m      4\u001b[39m json_path = \u001b[33m\"\u001b[39m\u001b[33m/data/raw/bird_map_as_json.json\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'main'"
     ]
    }
   ],
   "source": [
    "from main import get_filename\n",
    "from transform_helpers import json_to_dataframes\n",
    "\n",
    "json_path = \"/data/raw/bird_map_as_json.json\"\n",
    "input_filename = get_filename(\"/data/raw/bird_map_as_json.json\")\n",
    "\n",
    "(all_bird_series, bird_names) = json_to_dataframes(input_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3140cdc",
   "metadata": {},
   "source": [
    "We can look at the results of this and see both a series of X/Y coordinates and the list of bird names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e368bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(all_bird_series[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84afbbc0",
   "metadata": {},
   "source": [
    "Note the `-1` values. We're coercing `None` to be `-1` in this case, since it's\n",
    "also possible to have had data and simply seen `0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d13ab4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(bird_names[1:5])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "635ac214",
   "metadata": {},
   "source": [
    "The next thing we can do is peak at our data. Graphs that look almost like barcodes have lots of -1s, aka years with no data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15c53743",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotters import render_bird_graphs\n",
    "\n",
    "render_bird_graphs(all_bird_series, bird_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef53455",
   "metadata": {},
   "source": [
    "### Selecting cluster count (_k_)\n",
    "\n",
    "The next step is to pick how many clusters we ultimately want to use. We can do\n",
    "this by using the \"elbow\" method, which basically means \"compute the inertias\n",
    "for each possible cluster size, then look at the graph and see where the \"elbow\"\n",
    "is in it (the point where adding more clusters is diminishing returns).\n",
    "\n",
    "To do this, we need to pick some maximal cluster count to work against. The\n",
    "absolute maximum would be the total number of series we have, which would be one\n",
    "cluster per dataset; not good. A general rule (that I can't find the source for)\n",
    "is to use the square root of sample size (i.e., the number of series we have\n",
    "total). You can always change up your maximal cluster count and see what works best.\n",
    "\n",
    "By eyeballing the graphs above, we conclude that we just don't have a clue about\n",
    "what the clusters might be.\n",
    "\n",
    "(A cool thing is that the elbow method is also used in compressing images -- you\n",
    "can pick the elbow for picking the minimum number of colors after which there's\n",
    "no real gain!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feb0ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotters import render_elbows\n",
    "import math\n",
    "\n",
    "# this calculation is arbitrary -- you could set your max cluster count to be 20, 100, etc\n",
    "max_cluster_count = int(math.sqrt(len(all_bird_series)))\n",
    "\n",
    "render_elbows(all_bird_series, max_cluster_count=max_cluster_count)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece2d673",
   "metadata": {},
   "source": [
    "If you run the elbow code above over and over, you'll see you get slightly different graphs.\n",
    "\n",
    "After some experimenting, we find that 6 seems pretty reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "453b6ab7",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_count = 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e87872d",
   "metadata": {},
   "source": [
    "### Clustering\n",
    "\n",
    "The next step, perhaps unsurprisingly, is to do the actual clustering.\n",
    "\n",
    "For this, since we're using timeseries data, we'll use a `TimeSeriesKMeans`. It takes a cluster count, and then a metric. The default metric is Euclidean, and the other two are \"dtw\" and \"softdtw\" where dtw stands for \"Dynamic Time-Warping\". We'll stick to Euclidean for now because the use-case for DTW usually involves timeseries where the time values differ and need to somehow be scaled into alignment. We have the same time values for every dataset.\n",
    "\n",
    "`fit_predict` both fits and predicts our data. \"Fitting\" (for kmeans) here means basically creating the cluster 'definitions' (explicitly, it computes the center of each cluster). \"Predicting\" here means actually assigning clusters to each data set based on. What we get as a result is the set of cluster labels -- a series of cluster indices that correspond to each dataset (and therefore to each bird)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9338a17",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "\n",
    "tskmeans = TimeSeriesKMeans(n_clusters=cluster_count, metric=\"dtw\")\n",
    "cluster_labels = tskmeans.fit_predict(all_bird_series)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65b2efdb",
   "metadata": {},
   "source": [
    "The first thing we can do is see how many things ended up in each cluster. This\n",
    "won't tell us the cluster contents, but is a neat way to see how the\n",
    "classification ended up."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbdc47a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotters import render_cluster_counts\n",
    "\n",
    "render_cluster_counts(cluster_count, cluster_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02686859",
   "metadata": {},
   "source": [
    "The next thing we can do to evaluate how well this worked is look at the bird data clustered together, by graphing each set in the cluster together."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "110ecdff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from plotters import render_clusters_with_barycenters\n",
    "\n",
    "render_clusters_with_barycenters(\n",
    "    cluster_labels=cluster_labels,\n",
    "    all_bird_series=all_bird_series,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04a98f4a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
