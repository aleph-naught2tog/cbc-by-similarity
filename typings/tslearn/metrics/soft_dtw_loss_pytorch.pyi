"""
This type stub file was generated by pyright.
"""

import torch
from torch.autograd import Function

"""The Soft-DTW loss function for PyTorch.

The Soft-DTW loss function for PyTorch proposed is inspired by Maghoumi's GitHub repository:
https://github.com/Maghoumi/pytorch-softdtw-cuda/blob/master/soft_dtw_cuda.py
"""
HAS_TORCH = ...
if not HAS_TORCH:
    class SoftDTWLossPyTorch:
        def __init__(self) -> None:
            ...
        
    
    
else:
    class _SoftDTWLossPyTorch(Function):
        """The Soft-DTW loss function."""
        @staticmethod
        def forward(ctx, D, gamma):
            """
            Parameters
            ----------
            ctx : context
            D : Tensor, shape=[b, m, n]
                Matrix of pairwise distances.
            gamma : float
                Regularization parameter.
                Lower is less smoothed (closer to true DTW).

            Returns
            -------
            loss : Tensor, shape=[batch_size,]
                The loss values.
            """
            ...
        
        @staticmethod
        def backward(ctx, grad_output): # -> tuple[Any, None, None]:
            ...
        
    
    
    class SoftDTWLossPyTorch(torch.nn.Module):
        r"""Soft-DTW loss function in PyTorch.

        Soft-DTW was originally presented in [1]_ and is
        discussed in more details in our
        :ref:`user-guide page on DTW and its variants<dtw>`.

        Soft-DTW is computed as:

        .. math::

            \text{soft-DTW}_{\gamma}(X, Y) =
                \min_{\pi}{}^\gamma \sum_{(i, j) \in \pi} d \left( X_i, Y_j \right)

        where :math:`d` is a distance function or a dissimilarity measure
        supporting PyTorch automatic differentiation and :math:`\min^\gamma` is
        the soft-min operator of parameter :math:`\gamma` defined as:

        .. math::

            \min{}^\gamma \left( a_{1}, ..., a_{n} \right) =
                - \gamma \log \sum_{i=1}^{n} e^{- a_{i} / \gamma}

        In the limit case :math:`\gamma = 0`, :math:`\min^\gamma` reduces to a
        hard-min operator. The soft-DTW is then defined as the square of the DTW
        dissimilarity measure when :math:`d` is the squared Euclidean distance.

        Contrary to DTW, soft-DTW is not bounded below by zero, and we even have:

        .. math::

            \text{soft-DTW}_{\gamma}(X, Y) \rightarrow - \infty \text{ when } \gamma \rightarrow + \infty

        In [2]_, new dissimilarity measures are defined, that rely on soft-DTW.
        In particular, soft-DTW divergence is introduced to counteract the non-positivity of soft-DTW:

        .. math::
            D_{\gamma} \left( X, Y \right) =
                \text{soft-DTW}_{\gamma}(X, Y)
                - \frac{1}{2} \left( \text{soft-DTW}_{\gamma}(X, X) + \text{soft-DTW}_{\gamma}(Y, Y) \right)

        This divergence has the advantage of being minimized for :math:`X = Y`
        and being exactly 0 in that case.

        Parameters
        ----------
        gamma : float
            Regularization parameter.
            It should be strictly positive.
            Lower is less smoothed (closer to true DTW).
        normalize : bool
            If True, the Soft-DTW divergence is used.
            The Soft-DTW divergence is always positive.
            Optional, default: False.
        dist_func : callable
            Distance function or dissimilarity measure.
            It takes two input arguments of shape (batch_size, ts_length, dim).
            It should support PyTorch automatic differentiation.
            Optional, default: None
            If None, the squared Euclidean distance is used.

        Examples
        --------
        >>> import torch
        >>> from tslearn.metrics import SoftDTWLossPyTorch
        >>> soft_dtw_loss = SoftDTWLossPyTorch(gamma=0.1)
        >>> x = torch.zeros((4, 3, 2), requires_grad=True)
        >>> y = torch.arange(0, 24).reshape(4, 3, 2)
        >>> soft_dtw_loss_mean_value = soft_dtw_loss(x, y).mean()
        >>> print(soft_dtw_loss_mean_value)
        tensor(1081., grad_fn=<MeanBackward0>)
        >>> soft_dtw_loss_mean_value.backward()
        >>> print(x.grad.shape)
        torch.Size([4, 3, 2])
        >>> print(x.grad)
        tensor([[[  0.0000,  -0.5000],
                 [ -1.0000,  -1.5000],
                 [ -2.0000,  -2.5000]],
        <BLANKLINE>
                [[ -3.0000,  -3.5000],
                 [ -4.0000,  -4.5000],
                 [ -5.0000,  -5.5000]],
        <BLANKLINE>
                [[ -6.0000,  -6.5000],
                 [ -7.0000,  -7.5000],
                 [ -8.0000,  -8.5000]],
        <BLANKLINE>
                [[ -9.0000,  -9.5000],
                 [-10.0000, -10.5000],
                 [-11.0000, -11.5000]]])

        See Also
        --------
        soft_dtw : Compute Soft-DTW metric between two time series.
        cdist_soft_dtw : Compute cross-similarity matrix using Soft-DTW metric.
        cdist_soft_dtw_normalized : Compute cross-similarity matrix using a normalized
            version of the Soft-DTW metric.

        References
        ----------
        .. [1] Marco Cuturi & Mathieu Blondel. "Soft-DTW: a Differentiable Loss Function for
           Time-Series", ICML 2017.
        .. [2] Mathieu Blondel, Arthur Mensch & Jean-Philippe Vert.
            "Differentiable divergences between time series",
            International Conference on Artificial Intelligence and Statistics, 2021.
        """
        def __init__(self, gamma=..., normalize=..., dist_func=...) -> None:
            ...
        
        def forward(self, x, y):
            """Compute the soft-DTW value between X and Y.

            Parameters
            ----------
            x : Tensor, shape=[batch_size, ts_length, dim]
                Batch of time series.
            y : Tensor, shape=[batch_size, ts_length, dim]
                Batch of time series.

            Returns
            -------
            loss : Tensor, shape=[batch_size,]
                The loss values.
            """
            ...
        
    
    
